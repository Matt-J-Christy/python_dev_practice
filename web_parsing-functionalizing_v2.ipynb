{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping and Writing to Google Sheets\n",
    "    1. Get the data and parse with requests, lxml and Beatiful Soup \n",
    "    2. Data wrange into dictionaries \n",
    "    3. Create Pandas DF and clean the data\n",
    "    4. Write to Google Sheets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading scraping packages and getting pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get \n",
    "import lxml.html as lh\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing google sheets writing packages and declaring credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import gspread \n",
    "#Service client credential from oauth2client\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "# Print nicely\n",
    "import pprint\n",
    "#Create scope\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "#create some credential using that scope and content of startup_funding.json\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('quickstart/g_sheet_creds.json',scope)\n",
    "#create gspread authorize using that credential\n",
    "client = gspread.authorize(creds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of URLs for scraping and writing, and loading helper function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://www.nfl.com/stats/weeklyleaders'\n",
    "\n",
    "weeks = [x for x in range(1,17)]\n",
    "\n",
    "stat_cat = ['Passing', 'Rushing', 'Receiving']\n",
    "\n",
    "urls = []\n",
    "\n",
    "names =[]\n",
    "\n",
    "for i in weeks:\n",
    "    for j in stat_cat:\n",
    "        full_url = base_url + '?week=' + str(i) + '&season=2019' + '&showCategory=' + j\n",
    "        urls.append(full_url)\n",
    "        name = '2019_' + 'week_' + str(i) + '_' + j\n",
    "        names.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding validation with pydantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, HttpUrl\n",
    "\n",
    "class url_model(BaseModel):\n",
    "    url: HttpUrl\n",
    "\n",
    "class nfl_query_model(BaseModel):\n",
    "    week: int\n",
    "    stat_category: str\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runing the validation on urls \n",
    "for i in range(0, len(urls)):\n",
    "    url_model(url = urls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running validation on query \n",
    "for i in range(0, 17):\n",
    "    for j in stat_cat:\n",
    "        nfl_query_model(week = i, stat_category = j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping NFL.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scrape_helper.py \n",
    "\n",
    "def scraper(page_url, sheet_name, share_mail): \n",
    "    \n",
    "    my_header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36'}\n",
    "    \n",
    "    #grabbing the HTML and getting text \n",
    "    fantasy_page = get(page_url, headers = my_header)\n",
    "\n",
    "    doc = lh.fromstring(fantasy_page.content)\n",
    "    \n",
    "    print(fantasy_page)\n",
    "    \n",
    "    #parsing table elements in the HTML inside the pattern \"//tr\" --> this is a table element \n",
    "\n",
    "    table_elements = doc.xpath('//tr')\n",
    "    \n",
    "    #getting column names \n",
    "    title = doc.xpath('//tr//th')\n",
    "    \n",
    "    colnames = []\n",
    "\n",
    "    n = len(title)\n",
    "\n",
    "    for i in range(0, n):\n",
    "        name = title[i].text_content()\n",
    "        colnames.append(name)\n",
    "            \n",
    "  #creating an empty array to insert the table elements \n",
    "    cols = []\n",
    "\n",
    "    i = 0 #setting the increment \n",
    "\n",
    "    for j in range(0, len(colnames)):\n",
    "        i+1\n",
    "        name = colnames[j] #getting the column name from the HTML table \n",
    "        #print('%d:\"%s\"'% (i, name))\n",
    "        cols.append((name, []))\n",
    "\n",
    "   #Since out first row is the header, data is stored on the second row onwards\n",
    "\n",
    "    for j in range(1,len(table_elements)):\n",
    "        #T is our j'th row\n",
    "        T=table_elements[j]\n",
    "\n",
    "        #If row is not of size 24, the //tr data is not from our table \n",
    "        if len(T)!=12:\n",
    "            break\n",
    "\n",
    "        #i is the index of our column\n",
    "        i=0\n",
    "\n",
    "        #Iterate through each element of the row\n",
    "        for t in T.iterchildren():\n",
    "            data=t.text_content() \n",
    "\n",
    "            #Append the data to the empty list of the i'th column\n",
    "            cols[i][1].append(data)\n",
    "            #Increment i for the next column\n",
    "            i+=1\n",
    "        \n",
    "    #creating a dictionary for the columns in the parsed table \n",
    "    Dict={title:column for (title,column) in cols}\n",
    "\n",
    "    df=pd.DataFrame(Dict)\n",
    "    \n",
    "    #data cleaning \n",
    "    escapes = ''.join([chr(char) for char in range(1, 32)])\n",
    "    translator = str.maketrans('', '', escapes)\n",
    "    df.columns = df.columns.str.translate(translator)\n",
    "    \n",
    "    #fixing escape charaters\n",
    "    fixed = ['Name', 'Team', 'Opp', 'Score']\n",
    "    for i in fixed:\n",
    "        df.loc[:, i] = df.loc[:, i].astype(str).str.translate(translator)\n",
    "        \n",
    "    #Grapping Parameters for looping \n",
    "    n_rows = df.shape[0]\n",
    "    n_cols = df.shape[1]\n",
    "    \n",
    "    #writing to google sheets \n",
    "    import time \n",
    "\n",
    "    #Now will can access our google sheets we call client.open on StartupName\n",
    "    sheet = client.create(sheet_name) #2019-q4_fantasy-web-scraping/passing\n",
    "\n",
    "    sheet.share(share_mail,  perm_type='user', role='writer') #sharing my email \n",
    "    \n",
    "    #writing data to the worksheet\n",
    "    ws = sheet.get_worksheet(0)\n",
    "\n",
    "    shaped_data = df.transpose()\n",
    "\n",
    "    ws.insert_row(df.columns.tolist(), 1)\n",
    "\n",
    "    for i in range(1, n_rows+1): \n",
    "        row = df.iloc[i-1].tolist()\n",
    "        index = i+1\n",
    "        if i%10 == 0: #printing the step in the loop\n",
    "            print(i)  \n",
    "            time.sleep(15)\n",
    "            \n",
    "        ws.insert_row(row, index) #writing the data \n",
    "    \n",
    "    print('row ', i, ' end of file')\n",
    "    time.sleep(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping throught the pages and writing to google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "10\n",
      "20\n",
      "30\n",
      "row  38  end of file\n",
      "2019_week_1_Passing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_1_Rushing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_1_Receiving\n",
      "<Response [200]>\n",
      "10\n",
      "20\n",
      "30\n",
      "row  38  end of file\n",
      "2019_week_2_Passing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_2_Rushing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_2_Receiving\n",
      "<Response [200]>\n",
      "10\n",
      "20\n",
      "30\n",
      "row  34  end of file\n",
      "2019_week_3_Passing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_3_Rushing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_3_Receiving\n",
      "<Response [200]>\n",
      "10\n",
      "20\n",
      "30\n",
      "row  38  end of file\n",
      "2019_week_4_Passing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_4_Rushing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_4_Receiving\n",
      "<Response [200]>\n",
      "10\n",
      "20\n",
      "30\n",
      "row  39  end of file\n",
      "2019_week_5_Passing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_5_Rushing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_5_Receiving\n",
      "<Response [200]>\n",
      "10\n",
      "20\n",
      "30\n",
      "row  31  end of file\n",
      "2019_week_6_Passing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_6_Rushing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_6_Receiving\n",
      "<Response [200]>\n",
      "10\n",
      "20\n",
      "30\n",
      "row  36  end of file\n",
      "2019_week_7_Passing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_7_Rushing\n",
      "<Response [200]>\n",
      "row  Score  end of file\n",
      "2019_week_7_Receiving\n",
      "<Response [200]>\n",
      "10\n",
      "20\n",
      "30\n",
      "row  33  end of file\n"
     ]
    }
   ],
   "source": [
    "for j in range(0,len(urls)):\n",
    "\n",
    "    scraper(page_url = urls[j], sheet_name = names[j], share_mail = 'matthewjchristy66@gmail.com')\n",
    "    \n",
    "    print(names[j])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "10\n",
      "20\n",
      "30\n",
      "row  38  end of file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scraper(page_url = urls[0], sheet_name = 'test_sheet', share_mail = 'matthewjchristy66@gmail.com')\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
